import argparse
import sys
import torch
import triton
from aiter.ops.triton.gemm_a8w8_blockscale import gemm_a8w8_blockscale
from op_tests.triton.test_gemm_a8w8_blockscale import generate_gemm_a8w8_blockscale_inputs
from utils.benchmark_utils import get_model_configs, get_available_models


block_shape = (128, 128)


def model_benchmark_shapes(args):
    config_file = args.model_configs
    configs = get_model_configs(config_path=config_file, models=args.model)
    M_list = [args.M] if args.model == "all" else [2 ** i for i in range(0, 15)]
    shapes = []
    for M in M_list:
        for _, config in configs.items():
            N = config["intermediate_size"]
            K = config["hidden_size"]

            shapes.append((M, N, K))

    return shapes


def get_x_vals():
    x_vals = [
        (1, 1280, 8192),
        (32, 1280, 8192),
        (64, 1280, 8192),
        (128, 1280, 8192),
        (192, 1280, 8192),
        (256, 1280, 8192),
        (320, 1280, 8192),
        (512, 1280, 8192),
        (1024, 1280, 8192),
        (2048, 1280, 8192),
        (4096, 1280, 8192),
        (8192, 1280, 8192),
        (16384, 1280, 8192),
    ]
    return x_vals


def run_benchmark(args):
    assert not(args.shape and args.model) or not(args.shape and args.M), \
        "User can specify --shape or --model MODEL -M VAL exclusively"

    x_names = ['M', 'N', 'K']
    if args.model:
        x_vals_list = model_benchmark_shapes(args)
    elif args.shape:
        x_vals_list = [args.shape]
    else:
        x_vals_list = get_x_vals()

    if args.metric == 'time':
        ylabel = 'Time (ms)'
    elif args.metric == 'throughput':
        ylabel = 'Throughput (TFLOPS)'
    elif args.metric == 'bandwidth':
        ylabel = 'Bandwidth (GB/s)'
    else:
        raise NotImplementedError(f"{args.metric} is not supported")

    line_names = ["Triton"]
    line_vals = ['triton']
    benchmark = triton.testing.Benchmark(
        x_names=x_names, x_vals=x_vals_list,
        line_arg='provider', line_vals=line_vals, line_names=line_names,
        styles=[('green', '-')],
        ylabel=ylabel, plot_name='GEMM A8W8 Benchmark', args={"metric": args.metric})

    @triton.testing.perf_report([benchmark])
    def bench_gemm_a8w8_blockscale(M, N, K, metric, provider):
        block_shape_n, block_shape_k = block_shape
        # TODO: Remove this skip condition
        assert (N % block_shape_n == 0) and (K % block_shape_k == 0), \
        f"{N}N/{K}K sizes not aligned to {block_shape}"

        c_dtype = torch.bfloat16
        x, weight, x_scale, w_scale = \
            generate_gemm_a8w8_blockscale_inputs(M, N, K, block_shape_n, block_shape_k)
        # flops
        flops = 2.0 * M * N * K
        # memory transfer
        mem_read = (M * K) * x.element_size() + (N * K) * weight.element_size()
        mem_write = (M * N) * 2 # TODO: Fix for c_dtype != bf16
        mem = mem_read + mem_write

        ms = triton.testing.do_bench(
            lambda: gemm_a8w8_blockscale(x, weight, x_scale, w_scale, c_dtype),
            warmup=25, rep=100)

        # Return exactly one scalar depending on which metric is active
        if metric == 'time':
            return ms
        elif metric == 'throughput':
            tflops = flops / ms * 1e-9
            return tflops
        elif metric == 'bandwidth':
            bandwidth = mem / (ms * 1e-3) * 1e-9  # GB/s
            return bandwidth
        else:
            raise ValueError("Unknown metric: " + metric)

    bench_gemm_a8w8_blockscale.run(save_path=".", print_data=True)


def parse_args():
    parser = argparse.ArgumentParser(
        prog="Benchmark A8W8 GEMM",
        allow_abbrev=False,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    available_models = get_available_models()  # Dynamically load model names
    model_help = ("Model name to benchmark. Select from: [" + ", ".join(available_models) +
                  "]. Use 'all' to benchmark all models or leave blank for the default benchmark script.")
    parser.add_argument('--model-configs', type=str, default="utils/model_configs.json", help="Model config json file.")
    parser.add_argument('--model', type=str, help=model_help)
    parser.add_argument('-M', type=int, default=4096, help="M dim of model benchmark if only one model is under test")
    parser.add_argument("--shape", type=int, nargs=3, metavar=("M", "N", "K"), help="user-defined shape to benchmark")
    parser.add_argument("--metric", type=str, choices=["time", "throughput", "bandwidth"], default="throughput", help="metric to plot")
    args = parser.parse_args()
    return args


def main():
    args = parse_args()
    run_benchmark(args)


if __name__ == '__main__':
    sys.exit(main())
